---
title: What Mechanisms Does Knowledge Distillation Distill?
openreview: https://openreview.net/forum?id=uVZB637Dgx
abstract: Knowledge distillation is a commonly-used compression method in ML due to
  the popularity of increasingly large-scale models, but it is unclear if all the
  information a teacher model contains is distilled into the smaller student model.
  We aim to formalize the concept of ‘knowledge’ to investigate how knowledge is transferred
  during distillation, focusing on shared invariant outputs to counterfactual changes
  of dataset latent variables (we call these latents mechanisms). We define a student
  model to be a good stand-in model for a teacher if it shares the teacher’s learned
  mechanisms, and find that Jacobian matching and contrastive representation learning
  are viable methods by which to train such models. While these methods do not result
  in perfect transfer of mechanisms, we show they often improve student fidelity or
  mitigate simplicity bias (as measured by the teacher-to-student KL divergence and
  accuracy on various out-of-distribution test datasets), especially on datasets with
  spurious statistical correlations.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wu24a
month: 0
tex_title: What Mechanisms Does Knowledge Distillation Distill?
firstpage: 60
lastpage: 75
page: 60-75
order: 60
cycles: false
bibtex_author: Wu, Cindy and Lubana, Ekdeep Singh and Mlodozeniec, Bruno Kacper and
  Kirk, Robert and Krueger, David
author:
- given: Cindy
  family: Wu
- given: Ekdeep Singh
  family: Lubana
- given: Bruno Kacper
  family: Mlodozeniec
- given: Robert
  family: Kirk
- given: David
  family: Krueger
date: 2024-05-14
address:
container-title: 'Proceedings of UniReps: the First Workshop on Unifying Representations
  in Neural Models'
volume: '243'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 5
  - 14
pdf: https://proceedings.mlr.press/v243/wu24a/wu24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
