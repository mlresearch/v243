---
title: 'WavSpA: Wavelet Space Attention for Boosting Transformers’ Long Sequence Learning
  Ability'
openreview: yC6b3hqyf8
abstract: Transformer and its variants are fundamental neural architectures in deep
  learning. Recent works show that learning attention in the Fourier space can improve
  the long sequence learning capability of Transformers. We argue that wavelet transform
  shall be a better choice because it captures both position and frequency information
  with linear time complexity. Therefore, in this paper, we systematically study the
  synergy between wavelet transform and Transformers. We propose Wavelet Space Attention
  (WavSpA) that facilitates attention learning in a learnable wavelet coefficient
  space which replaces the attention in Transformers by (1) applying forward wavelet
  transform to project the input sequences to multi-resolution bases, (2) conducting
  attention learning in the wavelet coefficient space, and (3) reconstructing the
  representation in input space via backward wavelet transform. Extensive experiments
  on the Long Range Arena demonstrate that learning attention in the wavelet space
  using either fixed or adaptive wavelets can consistently improve Transformer’s performance
  and also significantly outperform learning in Fourier space. We further show our
  method can enhance Transformer’s reasoning extrapolation capability over distance
  on the LEGO chain-of-reasoning task.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhuang24a
month: 0
tex_title: 'WavSpA: Wavelet Space Attention for Boosting Transformers’ Long Sequence
  Learning Ability'
firstpage: 27
lastpage: 46
page: 27-46
order: 27
cycles: false
bibtex_author: Zhuang, Yufan and Wang, Zihan and Tao, Fangbo and Shang, Jingbo
author:
- given: Yufan
  family: Zhuang
- given: Zihan
  family: Wang
- given: Fangbo
  family: Tao
- given: Jingbo
  family: Shang
date: 2024-05-14
address:
container-title: 'Proceedings of UniReps: the First Workshop on Unifying Representations
  in Neural Models'
volume: '243'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 5
  - 14
pdf: https://proceedings.mlr.press/v243/zhuang24a/zhuang24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
