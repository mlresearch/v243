---
title: 'ReWaRD: Retinal Waves for Pre-Training Artificial Neural Networks Mimicking
  Real Prenatal Development'
openreview: rNiKAZifvI
abstract: Computational models trained on a large amount of natural images are the
  state-of-the-art to study human vision – usually adult vision. Computational models
  of infant vision and its further development are gaining more and more attention
  in the community. In this work we aim at the very beginning of our visual experience
  – pre- and post-natal retinal waves which suggest to be a pre-training mechanism
  for the human visual system at a very early stage of development. We see this approach
  as an instance of biologically plausible data driven inductive bias through pre-training.
  We built a computational model that mimics this development mechanism by pre-training
  different artificial convolutional neural networks with simulated retinal wave images.
  The resulting features of this biologically plausible pre-training closely match
  the V1 features of the human visual system. We show that the performance gain by
  pre-training with retinal waves is similar to a state-of-the art pre-training pipeline.
  Our framework contains the retinal wave generator, as well as a training strategy,
  which can be a first step in a curriculum learning based training diet for various
  models of development. We release code, data and trained networks to build the basis
  for future work on visual development and based on a curriculum learning approach
  including prenatal development to support studies of innate vs. learned properties
  of the human visual system. An additional benefit of our pre-trained networks for
  neuroscience or computer vision applications is the absence of biases inherited
  from datasets like ImageNet.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cappell24a
month: 0
tex_title: 'ReWa{RD}: Retinal Waves for Pre-Training Artificial Neural Networks Mimicking
  Real Prenatal Development'
firstpage: 76
lastpage: 86
page: 76-86
order: 76
cycles: false
bibtex_author: Cappell, Benjamin and Stoll, Andreas and Umah, Chukwudi Williams and
  Egger, Bernhard
author:
- given: Benjamin
  family: Cappell
- given: Andreas
  family: Stoll
- given: Chukwudi Williams
  family: Umah
- given: Bernhard
  family: Egger
date: 2024-05-14
address:
container-title: 'Proceedings of UniReps: the First Workshop on Unifying Representations
  in Neural Models'
volume: '243'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 5
  - 14
pdf: https://proceedings.mlr.press/v243/cappell24a/cappell24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
