---
title: Multimodal decoding of human brain activity into images and text
openreview: https://openreview.net/forum?id=rGCabZfV3d
abstract: Every day, the human brain processes an immense volume of visual information,
  relying on intricate neural mechanisms to perceive and interpret these stimuli.
  Recent breakthroughs in functional magnetic resonance imaging (fMRI) have enabled
  scientists to extract visual information from human brain activity patterns. In
  this study, we present an innovative method for decoding brain activity into meaningful
  images and captions, with a specific focus on brain captioning due to its enhanced
  flexibility as compared to brain decoding into images. Our approach takes advantage
  of cutting-edge image captioning models and incorporates a unique image reconstruction
  pipeline that utilizes latent diffusion models and depth estimation.  We utilized
  the Natural Scenes Dataset, a comprehensive fMRI dataset from eight subjects who
  viewed images from the COCO dataset. We employed the Generative Image-to-text Transformer
  (GIT) as our backbone for captioning and propose a new image reconstruction pipeline
  based on latent diffusion models. The method involves training regularized linear
  regression models between brain activity and extracted features. Additionally, we
  incorporated depth maps from the ControlNet model to further guide the reconstruction
  process. We propose a multimodal based approach that leverage similarities between
  neural and deep learning presentation and by learning alignment between these spaces,
  we produce textual description and image reconstruction from brain activity. We
  evaluate our methods using quantitative metrics for both generated captions and
  images. Our brain captioning approach outperforms existing methods, while our image
  reconstruction pipeline generates plausible images with improved spatial relationships.  In
  conclusion, we demonstrate significant progress in brain decoding, showcasing the
  enormous potential of integrating vision and language to better understand human
  cognition. Our approach provides a flexible platform for future research, with potential
  applications based on a combination of high-level semantic information coming from
  text and low-level image shape information coming from depth maps and initial guess
  images.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ferrante24a
month: 0
tex_title: Multimodal decoding of human brain activity into images and text
firstpage: 87
lastpage: 101
page: 87-101
order: 87
cycles: false
bibtex_author: Ferrante, Matteo and Boccato, Tommaso and Ozcelik, Furkan and VanRullen,
  Rufin and Toschi, Nicola
author:
- given: Matteo
  family: Ferrante
- given: Tommaso
  family: Boccato
- given: Furkan
  family: Ozcelik
- given: Rufin
  family: VanRullen
- given: Nicola
  family: Toschi
date: 2024-05-14
address:
container-title: 'Proceedings of UniReps: the First Workshop on Unifying Representations
  in Neural Models'
volume: '243'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 5
  - 14
pdf: https://proceedings.mlr.press/v243/ferrante24a/ferrante24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
