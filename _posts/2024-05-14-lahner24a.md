---
title: On the Direct Alignment of Latent Spaces
openreview: https://openreview.net/forum?id=nro8tEfIfw
abstract: With the wide adaption of deep learning and pre-trained models rises the
  question of how to effectively reuse existing latent spaces for new applications.One
  important question is how the geometry of the latent space changes in-between different
  training runs of the same architecture and different architectures trained for the
  same task. Previous works proposed that the latent spaces for similar tasks are
  approximately isometric. However, in this work we show that method restricted to
  this assumption perform worse than when just using a linear transformation to align
  the latent spaces. We propose directly computing a transformation between the latent
  codes of different architectures which is more efficient than previous approaches
  and flexible wrt. to the type of transformation used. Our experiments show that
  aligning the latent space with a linear transformation performs best while not needing
  more prior knowledge.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lahner24a
month: 0
tex_title: On the Direct Alignment of Latent Spaces
firstpage: 158
lastpage: 169
page: 158-169
order: 158
cycles: false
bibtex_author: L\"ahner, Zorah and Moeller, Michael
author:
- given: Zorah
  family: LÃ¤hner
- given: Michael
  family: Moeller
date: 2024-05-14
address:
container-title: 'Proceedings of UniReps: the First Workshop on Unifying Representations
  in Neural Models'
volume: '243'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 5
  - 14
pdf: https://proceedings.mlr.press/v243/lahner24a/lahner24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
