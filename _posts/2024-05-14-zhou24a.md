---
title: Comparing neural models using their perceptual discriminability predictions
openreview: https://openreview.net/forum?id=mDr0o2WU2n
abstract: 'A variety of methods have been developed to compare models of visual representation.
  However, internal representations are not uniquely identifiable from perceptual
  measurements: different representations can generate identical perceptual predictions,
  and dissimilar model representations (according to existing model comparison methods)
  do not guarantee dissimilar perceptual predictions. Here, we generalize a previous
  method (“eigendistortions” - Berardino et al, 2017) to compare models based on their
  metric tensors. Metric tensors characterize a model’s sensitivity to stimulus perturbations,
  reflecting both the geometric and stochastic properties of the representation, and
  providing an explicit prediction of perceptual discriminability. Brute force comparison
  of model-predicted metric tensors using human perceptual thresholds would require
  an impossibly large set of measurements, since one needs to perturb a stimulus in
  all possible orthogonal directions. To circumvent this “perceptual curse of dimensionality”,
  we compute and measure discrimination capabilities for a small set of most-informative
  perturbations, reducing the measurement cost from thousands of hours (a conservative
  estimate) to a single trial. We show that this single measurement, made for a variety
  of different test stimuli, is sufficient to differentiate models, select models
  that better match human perception, or generate new models that combine the advantages
  of both. We demonstrate the power of this method in assessing two examples: 1) comparing
  models for color discrimination; 2) comparing autoencoders trained with different
  regularizers.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhou24a
month: 0
tex_title: Comparing neural models using their perceptual discriminability predictions
firstpage: 170
lastpage: 181
page: 170-181
order: 170
cycles: false
bibtex_author: Zhou, Jingyang and Chun, Chanwoo and Subramanian, Ajay and Simoncelli,
  Eero P
author:
- given: Jingyang
  family: Zhou
- given: Chanwoo
  family: Chun
- given: Ajay
  family: Subramanian
- given: Eero P
  family: Simoncelli
date: 2024-05-14
address:
container-title: 'Proceedings of UniReps: the First Workshop on Unifying Representations
  in Neural Models'
volume: '243'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 5
  - 14
pdf: https://proceedings.mlr.press/v243/zhou24a/zhou24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
